# Prep  
- [CDE](https://github.com/jvprosser/Iceberg-wap/blob/main/.script.md#cloudera-data-engineering)
- [Data Catalog](https://github.com/jvprosser/Iceberg-wap/blob/main/.script.md#data-catalog)
--------  

- Go to Hue
 ```
 DROP TABLE factset.reddit_fin_chat;
 ```

- Start a CDE session
- Add an airflow connector to your CDE VC for your CDW Hive VW and call it cdw-hive-demo
```
   Connection type = hive client wrapper
```
- connection
```
hs2-default-hive-aws
```
## Cloudera DataFlow
```
Cloudera DataFlow is a cloud-native data service that facilitates universal data distribution.

Based on Apache NiFi, Cloudera DataFlow  provides a no-code interface for moving and
enriching data between any source and any destination.

By Enriching, I mean making fast per-record adjustments to the data as it flows.
This could be routing,  converting data or even use a REST service for GEOTagging.

There are over 450 connectors enabling data movement no matter how the data is
structured or stored.

```
### Go to Deployments and deploy *FS-HuggingFace-practice*
```
Deployments represent an instance of a dataflow moving data from one place to another.
We'll be drilling into this deployment during this demo and we'll come back here in a few minutes.
```
### Go to Ready flow gallery and show all the options
`At a high level, these options show the basic functionality of Cloudera Data Flow`

### Go to flow design 
`This is the collection of flows that are in some stage of development`

- Click on FS-HuggingFace-dataset1-reddit-fin
- Go over the flow details
```
Each square is called a processor and represents a stage in the data movement process flow
```

- Go to the Parameters section and show how this can be reused.

###  Go to projects 
- `Projects are a functional grouping tool`
- search on `FS-` 
- Menu on right -> View project resources
- `These are all the instances of the flow representing different HuggingFace datasets that I want to download`

### Go to the catalog and type
```FS- ```
In the search field

- Click on FS-HuggingFace-dataset-Import


### Click on DEPLOY
- Give it a name
```
FS-HuggingFace-practice
```

- Target project
```
FS-
```

- Datasetname
- ```
  sebdg/trading_data
  ```

- Dest S3
- ```
  /tmp/jvp/practice
  ```

- Storage loc
- ```
  s3a://go01-demo
  ```

- Sizing & Scaling - GO WITH DEFAULTS

- KPIs Go with the entire flow *Flowfiles Queued*



# Cloudera Data Engineering
```
Cloudera Data Engineering (CDE) is a cloud-native service for building, deploying,
and managing data pipelines on the Cloudera Data Platform 
```
```
It's used for Automating data pipelines, developing and executing ETL and ELT processes,
and Data preparation for analytics and machine learning
```

```
Data Engineering uses  Apache Spark and PySpark for data processing,
Python, Scala, and Java for development,  and Apache Airflow for orchestration
```

### Jobs
```
Jobs are  a defined configuration, of resources and code and than be run on demand or scheduled.
This listing shows all the jobs that have been configured to run on this virtual cluster
```


### Administration
```
A CDE service is a kubernetes cluster within which one or more virtual clusters can be defined

A virtual cluster uses a subset of the service and allows for different configurations, such as spark version
```

### Job runs
```
The Job runs page shows the actvity on a given virtual cluster.
```
### Repositories
```
Git repositories allow teams to collaborate, manage project artifacts, and promote applications from lower to higher environments.

Cloudera currently supports Git providers such as GitHub, GitLab, and Bitbucket.
```
- Create a repository
```
FS-Repo
```
```
https://github.com/jvprosser/Iceberg-wap.git
```

- We'll use this repo later to create jobs and workflows


### Sessions

```
A Session is an interactive UI that can be used for iterative development and debugging.

It's operation is simplistic, because the intended interface is Jupyter, VSCode, or some other UI

```

### Here is an example.

```
from __future__ import print_function
import os
import sys
from pyspark.sql import SparkSession

data_lake_name= "s3a://go01-demo/" 

srcdir="/tmp/RedditFinance/winddude/reddit_finance_43_250k/parquet/default/train/0.parquet"
tablename="reddit_fin_chat"
database="factset" 
```
—--------------------
```
df = spark.read.options(header='True', inferSchema='True', delimiter=',') \
  .parquet(f"{data_lake_name}/{srcdir}")

df.printSchema()
```
—--------------------
## FIRST TIME
```
df.writeTo(f"{database}.{tablename}")\
     .tableProperty("write.format.default", "parquet")\
     .using("iceberg")\
     .createOrReplace()
```
—--------------------
```
spark.sql(f"SELECT * FROM {database}.{tablename}").show(10)

print ("Getting row count")

spark.sql(f"SELECT count(*) FROM {database}.{tablename}").show(10)
```
—--------------------

```
print ("List Iceberg Snapshots")

spark.sql(f"SELECT * FROM {database}.{tablename}.snapshots").show()
print ("List Iceberg References")

spark.sql(f"SELECT * FROM {database}.{tablename}.refs").show()
```

# Data Catalog

- Go to datacatalog and type in factset

- click on factset.db hive DB

- Click on iceberg tables and reddit_fin_chat

- Schema -> Edit classifications add ```SensitiveData``` to the ID field.

- Click on link to experience at the top and go to hive.
=== CDW
```
A Virtual Warehouse is an instance of compute resources, equivalent to a cluster.


```
```
SELECT *
FROM factset.reddit_fin_chat
LIMIT 100
;
```
generate some SQL with the assistant
```how many messages are about factset```


### Go back to CDE
- Create a Job called *FS-Ingest*
- Application files from Repository created earlier
- Select *dowap.py*
- Go and look at the [code](https://github.com/jvprosser/Iceberg-wap/blob/main/dowap.py) and talk about Write-Audit-Publish and Tags
1. Add Arguments
- Source Dir
```
/tmp/RedditFinance/winddude/reddit_finance_43_250k/parquet/default/train/1.parquet
```
- Table
```
reddit_fin_chat
```
- Database
```
factset
```
  2. Create and Run
- Go and look at the Spark UI and logs etc.
-----------------
###  Go to CDW
-  List Snapshots
-  ```
   SELECT * FROM factset.reddit_fin_chat.snapshots
   ```
-  we want to re run this ingest but as an airflow job so we will roll back this operation
```
ALTER TABLE factset.reddit_fin_chat EXECUTE ROLLBACK(PUT_YOUR_SNAPSHOT_HERE); 
```

### Go back to CDE
  
- Create an Airflow job
- Add a shell script
- Add a CDE job
- Check ===Override Spark values===
*Source Dir*
```
/tmp/RedditFinance/winddude/reddit_finance_43_250k/parquet/default/train/1.parquet
```
*Table*
```
reddit_fin_chat
```
*Database*
```
factset
```
- Add a CDW query using `cdw_hive_default_iceberg` Virtual warehouse connection

```
select * from factset.reddit_fin_chat limit 11
```
- Add a python Script
 ```
  print("Job Finished")
  ````
- Run the job
=== to back to CDW
While that job is running we'll look at some queries
```
 select * FROM factset.reddit_fin_chat.refs;
 select * FROM factset.reddit_fin_chat.history;
```
